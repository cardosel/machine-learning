{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "When it comes to tasks involving many features that are nonlinear, like in the example of images, approaching this kind of classification task with logistic regression becomes quite computationally expensive.\n",
    "\n",
    "For example, let's say we have an image that's 10x10 pixels. This would give us a total of 100 features. Since we're assuming the data is nonlinear, we would be required to add more polynomials with a higher order like a quadratic. To do this, you'd have $x_1 + x_2 + x_3 + ... + x_{100}$ in addition to the quadratic term $x_1^2 + x_2^2 + x_3^2 + ... + x_{100}^2$ plus all the unique combinations of pairwise products, i.e. $x_1 * x_2,\\ x_1 * x_3,\\ x_1 * x_4...$. Turns out you'd have around 5,000 features when you add all this up ($\\approx \\frac{n^2}{2}$). Extending this to a cubic function, you'd have around 170,000 features.\n",
    "\n",
    "As you can see, the addition of all these various polynomials can add up quick. Imagine if we had an image that was 100x100. Being computationally expensive, that logistic model would take some time and even risk overfitting. So what does a neural network moel look like?\n",
    "\n",
    "<img src=\"neural-network.png\" width=50%, height=50% />\n",
    "\n",
    "$x_1$, $x_2$, and $x_3$ in this case are input variables and comprise the first layer called the \"input layer\" of this neural network. If this were an image, we might have 100 variables here. Layer two comprising of $a_1^{(2)}$, $a_2^{(2)}$, and $a_3^{(2)}$ is called the \"hidden layer\". The last layer is called the \"output layer\" and in the picture above, only consists of one \"neuron\" or \"unit\". This last layer is equivalent to the output of our hypothesis $h_\\theta(x)$.\n",
    "\n",
    "For our input layer, we might have a vector of input values like so: $X = [x_0, x_1, x_2, x_3]$. We have to add $x_0$ as the \"y-intercept\" like we had to do in linear and logistic regression; for neural networks, we call this y-intercept the \"bias unit\".\n",
    "\n",
    "We also must have a vector for our weights/thetas: $\\theta = [\\theta_0, \\theta_1, \\theta_2, \\theta_3]$.\n",
    "\n",
    "The idea goes like this: before the input values get to layer 2, they are multiplied by their respective weights. After this, they are run through the \"activation units\" in layer 2. The activation function used here is actually a function we're already familiar with - the sigmoid function.\n",
    "\n",
    "## Notation\n",
    "\n",
    "$a_i^{(j)}$ = activation of unit $i$ in layer $j$\n",
    "\n",
    "$\\theta^{(j)}$ = matrix of weights controlling function mapping from layer $j$ to layer $j + 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedforward\n",
    "\n",
    "The below is how we'd calculate the activation function for each unit in layer 2.\n",
    "\n",
    "$a_1^{(2)} = g(\\theta_{10}^{(1)}x_0 + \\theta_{11}^{(1)}x_1 + \\theta_{12}^{(1)}x_2 + \\theta_{13}^{(1)}x_3)$\n",
    "\n",
    "$a_2^{(2)} = g(\\theta_{20}^{(1)}x_0 + \\theta_{21}^{(1)}x_1 + \\theta_{22}^{(1)}x_2 + \\theta_{23}^{(1)}x_3)$\n",
    "\n",
    "$a_3^{(2)} = g(\\theta_{30}^{(1)}x_0 + \\theta_{31}^{(1)}x_1 + \\theta_{32}^{(1)}x_2 + \\theta_{33}^{(1)}x_3)$\n",
    "\n",
    "In the the last layer (the output layer) we'll do the same thing as above.\n",
    "\n",
    "$h_\\theta(x) = a_1^{(3)} = g(\\theta_{10}^{(2)}a_0^{(2)} + \\theta_{11}^{(2)}a_1^{(2)} + \\theta_{12}^{(2)}a_2^{(2)} + \\theta_{13}^{(2)}a_3^{(2)})$\n",
    "\n",
    "## Feedforward with Alternate Notation\n",
    "\n",
    "Let's define $\\theta_{10}^{(1)}x_0 + \\theta_{11}^{(1)}x_1 + \\theta_{12}^{(1)}x_2 + \\theta_{13}^{(1)}x_3$ as $z_1^{(2)}$ so that we can define $a_1^{(2)} = g(z_1^{(2)})$. This should make the above lines in the feedforward section a little easier to swallow.\n",
    "\n",
    "We can write all the linear functions as a vector for a single layer like below.\n",
    "\n",
    "$z^{(2)} = [z_1^{(2)}, z_2^{(2)}, z_3^{(2)}]$ where $z^{(2)} = \\theta^{(1)}a^{(1)}$ and $x = a^{(1)}$\n",
    "\n",
    "$z^{(3)} = \\theta^{(2)}a^{(2)}$\n",
    "\n",
    "We can easily write the activation piece in a vectorized form, making notation easier.\n",
    "\n",
    "$a^{(2)} = g(z^{(2)})$\n",
    "\n",
    "$h_\\theta(x) = a^{(3)} = g(z^{(3)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Cost Function\n",
    "\n",
    "$L$ = total number of layers in network\n",
    "\n",
    "$s_l$ = number of units (not counting bias unit) in layer $l$\n",
    "\n",
    "$K$ = number of classes\n",
    "\n",
    "$s_L = K$ use $K$ if $K \\ge 3$, else use binary classification (i.e. $y = 0$ or $1$)\n",
    "\n",
    "Let's remember what the cost function when we built of logistic regression model (with the addition of the regularization term).\n",
    "\n",
    "$$J(\\theta) = -\\frac{1}{m}[\\sum\\limits_{i=1}^m y^{(i)}\\log(h_\\theta(x^{(i)}) + (1 - y^{(i)})\\log(1 - h_\\theta(x^{(i)}))] + \\frac{\\lambda}{2m}\\sum\\limits_{j=1}^n \\theta_j^2$$\n",
    "\n",
    "Using the logistic cost function, we can derive the cost function for our neural network.\n",
    "\n",
    "$$J(\\theta) = -\\frac{1}{m}[\\sum\\limits_{i=1}^m\\sum\\limits_{k=1}^K y^{(i)}\\log(h_\\theta(x^{(i)})_k + (1 - y_k^{(i)})\\log(1 - h_\\theta(x^{(i)})_k)] + \\frac{\\lambda}{2m}\\sum\\limits_{l=1}^{L-1} \\sum\\limits_{i=1}^{s_l}\\sum\\limits_{j=1}^{s_l+1}(\\theta_{ji}^{(l)})^2$$\n",
    "\n",
    "We can minimize the above cost function by using the \"backpropagation\" algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "Intuition: $\\delta_j^{(l)} = $ \"error\" of node $j$ in layer $l$.\n",
    "\n",
    "For each output unit (layer $L = 4$)\n",
    "\n",
    "$\\delta_j^{(4)} = a_j^{(4)} - y_j$ which is the same as $\\delta^{(4)} = a^{(4)} - y$\n",
    "\n",
    "$\\delta^{(3)} = (\\theta^{(3)})^T\\delta^{(4)} * g'(z^{(3)})$ where $g'(z^{(3)})$ is $a^{(3)} * (1 - a^{(3)})$ and $1$ is a vector of ones\n",
    "\n",
    "$\\delta^{(2)} = (\\theta^{(2)})^T\\delta^{(3)} * g'(z^{(2)})$\n",
    "\n",
    "There's no $\\delta^{(1)}$ term since that would just corresponding to our input features and we don't really want to try to change these values.\n",
    "\n",
    "So our algorithm is like so:\n",
    "set $\\Delta_{ij}^{(l)} = 0$ for all $l$, $i$, and $j$\n",
    "     \n",
    "For $i = 1$ to $m$:\n",
    "1. $a^{(i)} = x^{(i)}$\n",
    "2. perform forward propogation to compute $a^{(l)}$ for $l = 2, 3, ..., L$\n",
    "3. using $y^{(i)}$, compute $\\delta^{(L)} = a^{(L)} - y^{(i)}$\n",
    "4. compute $\\delta^{(L-1)}, \\delta^{(L-2)}, ..., \\delta^{(2)}$\n",
    "5. $\\Delta_{ij}^{(l)} = \\Delta_{ij}^{(l)} + a_j^{(l)}\\delta_i^{(l+1)}$\n",
    "\n",
    "After doing the above, compute:\n",
    "\n",
    "$D_{ij}^{(l)} = \\frac{1}{m}\\Delta_{ij}^{(l)} + \\lambda\\theta_{ij}^{(l)}$ if $j \\ne 0$\n",
    "\n",
    "$D_{ij}^{(l)} = \\frac{1}{m}\\Delta_{ij}^{(l)}$ if $j = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is an implementation of a neural network using the above math.\n",
    "This code is a work in progress and isn't complete, check back in for updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# libraries\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BackPropagationNetwork:\n",
    "    \"\"\"a back-propagation network\"\"\"\n",
    "    # class members\n",
    "    layerCount = 0\n",
    "    shape = None\n",
    "    weights = []\n",
    "    \n",
    "    # class methods\n",
    "    def __init__(self, layerSize):\n",
    "        \"\"\"initialize the network\"\"\"\n",
    "        \n",
    "        # layer info\n",
    "        self.layerCount = len(layerSize) - 1 # - 1 to ignore input layer\n",
    "        self.shape = layerSize\n",
    "        \n",
    "        # input/output data from last run\n",
    "        self._layerInput = []\n",
    "        self._layerOutput = []\n",
    "        \n",
    "        # create the weight arrays\n",
    "        for (l1, l2) in zip(layerSize[: -1], layerSize[1:]):\n",
    "            self.weights.append(np.random.normal(scale=0.1, size=(l2, l1 + 1)))\n",
    "    \n",
    "    def ForwardPropagation(self, train):\n",
    "        # assume format of input is a bunch of rows\n",
    "        num_of_observations = train.shape[0]\n",
    "        \n",
    "        # clear out previous values\n",
    "        self._layerInput = []\n",
    "        self._layerOutput = []\n",
    "        \n",
    "        # run it forward\n",
    "        for index in range(self.layerCount):\n",
    "            if index == 0:\n",
    "                layerInput = self.weights[0].dot(np.vstack([train.T, np.ones([1, num_of_observations])])) # like cbind in R\n",
    "            else:\n",
    "                layerInput = self.weights[index].dot(np.vstack([self._layerOutput[-1], np.ones([1, num_of_observations])]))\n",
    "            \n",
    "            self._layerInput.append(layerInput)\n",
    "            self._layerOutput.append(self.sigmoid(layerInput))\n",
    "            \n",
    "        return self._layerOutput[-1].T\n",
    "    \n",
    "    def TrainEpoch(self, train, target, alpha = 0.2):\n",
    "        delta = []\n",
    "        num_of_observations = train.shape[0]\n",
    "        \n",
    "        self.ForwardPropagation(train)\n",
    "        \n",
    "        # calculate our deltas\n",
    "        for index in reversed(range(self.layerCount)):\n",
    "            if index == self.layerCount - 1: # if dealing with the last layer\n",
    "                # compare to the target values\n",
    "                output_delta = self._layerOutput[index] - target.T\n",
    "                error = np.sum(output_delta ** 2)\n",
    "                delta.append(output_delta * self.sigmoid(self._layerInput[index], True))\n",
    "            else:\n",
    "                delta_pullback = self.weights[index + 1].T.dot(delta[-1])\n",
    "                # get all rows from delta_pullback except last one which corresponds to the bias unit\n",
    "                delta.append(delta_pullback[:-1, :] * self.sigmoid(self._layerInput[index], True))\n",
    "        \n",
    "    # activation function\n",
    "    def sigmoid(self, x, Derivative=False):\n",
    "        if not Derivative:\n",
    "            return 1 / (1+np.exp(-x))\n",
    "        else:\n",
    "            output = self.sigmoid(x)\n",
    "            return output * (1 - output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "network shape:\n",
      "(2, 2, 1)\n",
      "\n",
      "\n",
      "random initialized weights:\n",
      "[array([[ 0.05766801,  0.12094731, -0.16733876],\n",
      "       [ 0.28126458,  0.07797792,  0.02846301]]), array([[-0.07035069,  0.12956574, -0.14875553]]), array([[-0.15652265, -0.09868612, -0.24421564],\n",
      "       [ 0.16283466,  0.31425346,  0.1617748 ]]), array([[-0.03626619,  0.09575352,  0.09716804]]), array([[-0.15622311,  0.01537343,  0.01399258],\n",
      "       [ 0.02509109,  0.24621699,  0.01138761]]), array([[ 0.05858299, -0.01433706, -0.01607383]]), array([[ 0.05086416, -0.10299167, -0.07930586],\n",
      "       [-0.13851765, -0.10827432,  0.00469726]]), array([[-0.11290109,  0.02640645, -0.14459487]]), array([[ 0.18721692,  0.02597043, -0.11170286],\n",
      "       [ 0.05126974,  0.03860757, -0.16939242]]), array([[-0.19274102,  0.1297081 ,  0.08165026]])]\n",
      "\n",
      "\n",
      "Input: [[0 0]\n",
      " [1 1]]\n",
      "Output: [[ 0.47120942]\n",
      " [ 0.47328971]]\n"
     ]
    }
   ],
   "source": [
    "bpn = BackPropagationNetwork((2, 2, 1))\n",
    "print 'network shape:'\n",
    "print bpn.shape\n",
    "print '\\n'\n",
    "print 'random initialized weights:'\n",
    "print bpn.weights\n",
    "print '\\n'\n",
    "\n",
    "train = np.array([[0, 0], [1, 1]])\n",
    "yHat = bpn.ForwardPropagation(train)\n",
    "\n",
    "print \"Input: {0}\\nOutput: {1}\".format(train, yHat)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
