{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "***\n",
    "\n",
    "## One Class\n",
    "\n",
    "Logistic regression is used for answering yes and no questions or questions involving the odds of something happening. To do this, it uses the \"logistic\" function.\n",
    "\n",
    "The idea is that you want your hypothesis, $h_\\theta (x)$ to be $> 1$ or $< 0$ and this is what logistic regression does for us - it limits the domain of our hypothesis to be between 0 and 1 which is great as 0 can be \"no\" and 1 can be \"yes\" in answering any kind of binary question.\n",
    "To express the above, we can say:\n",
    "\n",
    "$$0 \\le h_\\theta (x) \\le 1$$\n",
    "\n",
    "Remember in the linear regression tutorial, we had defined our hypothesis, $h_\\theta (x)$ to be $\\theta^T x$. Now our new hypothesis for logistic regression is defined as:\n",
    "\n",
    "$$h_\\theta (x) = g(\\theta^T x)$$\n",
    "<p style=\"text-align:center\"> where </p>\n",
    "$$g(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "This $g(z)$ function is also called the \"sigmoid\" function or the \"logistic\" function as is graphed below.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/2000px-Logistic-curve.svg.png\" width=30%, height=30% />\n",
    "\n",
    "Thus, by substitution, we can say that our new hypothesis is:\n",
    "\n",
    "$$h_\\theta (x) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "To break the above equation down a little, $h_\\theta (x)$ will be our estimated probability that $y = 1$ on input $x$. This can be expressed as the probability that $y = 1$ given $x$ using parameters $\\theta$ or in other words:\n",
    "\n",
    "$$h_\\theta (x) = P(y = 1\\ |\\ x;\\ \\theta)$$\n",
    "\n",
    "Since we know that if the probability of something is $0.3$, then the probability of that \"not-something\" would be $1 - 0.3 = 0.7$. Remember that in probabilities, 0 means you're absolutely certain the event won't happen and 1 means you're absolutely sure that something will happen. That being said, if we wanted to find the probability when $y = 1$, we could express it as such:\n",
    "\n",
    "$$P(y = 0\\ |\\ x;\\ \\theta) = 1 - P(y = 1\\ |\\ x;\\ \\theta)$$\n",
    "\n",
    "So we will be predicting $y = 1$ if $\\theta^T x \\ge 0$. This makes sense looking at the graph of the sigmoid function. If our line ($\\theta^T x$) outputs something greater than 0, we can the sigmoid function asymptotically approaches 1.\n",
    "You can see that we're still using our line equation, $\\theta^T x$, in a way. If you takea another look at the graph and only look at the portion of the graph between -2 and 2, you can see the resemblance of a linear function. So the sigmoid function involves a linear function but is expressed in a way that we can use it for classification instead of just for predictions involving continuous numbers. This line we see in the graph is defined as the decision boundary for our model, separating the two different groups of data points.\n",
    "\n",
    "In the previous notebook about linear regression, we came up with a cost function for our hypothesis, but that cost function was convex.\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{N}\\sum\\limits_{i=1}^N(h_\\theta (x^{(i)}) - y^{(i)})^2$$\n",
    "\n",
    "<p style=\"text-align:center\"> where $\\theta$ is a vector of coefficients and $i$ is the ith training example </p>\n",
    "\n",
    "Our sigmoid function is non-linear, thus non-convex, so we can't use the same Mean Squared Error cost function as we did in linear regression since we need a cost function that's convex so our gradient descent algorithm can easily converge at the global minimum.\n",
    "\n",
    "Let's start out with rewriting the cost function like so:\n",
    "\n",
    "$$J(\\theta) = \\frac{1}{N}\\sum\\limits_{i=1}^N\\ Cost(h_\\theta (x^{(i)}), y^{(i)})$$\n",
    "\n",
    "We can define $Cost(h_\\theta (x^{(i)}), y^{(i)})$ as:\n",
    "\n",
    "$$Cost(h_\\theta (x), y) =\n",
    "\\begin{cases}\n",
    "-\\log(h_\\theta (x))\\ if\\ y = 1\\\\\n",
    "-log(1 - h_\\theta(x))\\ if\\ y = 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The first function of the piecewise function will be severely punished (high cost) if we predict 0 but $y = 1$. Inversely, if we predict 1 but y = 0 with the second function in the piecewise function, there will be a high cost (the model will be punished).\n",
    "\n",
    "We can actually simplify the piecewise function above to be:\n",
    "\n",
    "$$Cost(h_\\theta (x), y) = -y \\log(h_\\theta (x)) - (1 - y) \\log(1 - h_\\theta(x))$$\n",
    "\n",
    "Simplifying the piecewise function, we can now rewrite the cost function as:\n",
    "\n",
    "$$J(\\theta) = -\\frac{1}{N}[\\sum\\limits_{i=1}^N y^{(i)}\\log(h_\\theta(x^{(i)}) + (1 - y^{(i)}\\log(1 - h_\\theta(x^{(i)}))]$$\n",
    "\n",
    "Now that we have our cost function for the sigmoid function, we need to find the partial derivative of that cost function so that we can use gradient descent to minimize $\\theta$.\n",
    "\n",
    "$$\\frac{\\delta}{\\delta \\theta_j}J(\\theta) = \\frac{2}{N}\\sum\\limits_{i=1}^N(h_\\theta (x^{(i)}) - y^{(i)})x_j^{(i)}$$\n",
    "\n",
    "Now we have everything we need to use the gradient descent algorithm (which is below if you've forgotten)!\n",
    "\n",
    "$$\\theta_j = \\theta_j - \\alpha \\frac{\\delta}{\\delta \\theta_j} J(\\theta)$$\n",
    "\n",
    "### Kaggle Competition Titanic Dataset\n",
    "\n",
    "We'll be using the dataset that is provided by the Titanic competition hosted at Kaggle.\n",
    "The goal of this competition is to use the dataset to build a model that predicts who survived and who didn't.\n",
    "\n",
    "Let's take a look at the dataset now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex  Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male   22      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female   38      1   \n",
       "2                             Heikkinen, Miss. Laina  female   26      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female   35      1   \n",
       "4                           Allen, Mr. William Henry    male   35      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pylab inline\n",
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be skipping any kind of preprocessing or feature engineering as this document is meant to just focus on logistic regression.\n",
    "\n",
    "Since we're wanting to predict the Survived column, we need to choose a couple predictors; let's use Pclass and Sex. We can start with what would be the linear part of the sigmoid function:\n",
    "$$\\theta^Tx = \\theta_0 * x_0 + \\theta_1 * Pclass + \\theta_2 * Sex$$\n",
    "\n",
    "Now that we've defined the linear part of the model, we can just plug it in to our hypothesis:\n",
    "$$h_\\theta (x) = \\frac{1}{1 + e^{-\\theta_0 * x_0 + \\theta_1 * Pclass + \\theta_2 * Sex}}$$\n",
    "\n",
    "Since we've already coded the gradient descent algorithm in the linear regression tutorial, we'll just skip to using third-party libraries. If you'd like to code the above algorithms yourself, refer to the linear regression notebook for implementation details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.464195\n",
      "         Iterations 6\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "# add a column of 0s for the intercept\n",
    "train['intercept'] = 1.0\n",
    "\n",
    "# make the Sex column a category type\n",
    "from sklearn import preprocessing\n",
    "le_sex = preprocessing.LabelEncoder()\n",
    "train['Sex'] = le_sex.fit_transform(train['Sex'])\n",
    "\n",
    "# identify the independent variables\n",
    "columns = ['intercept', 'Pclass', 'Sex']\n",
    "logistic_function = sm.Logit(train['Survived'], train[columns])\n",
    "model_parameters = logistic_function.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out how our model adjusted the parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "intercept    3.294642\n",
       "Pclass      -0.960553\n",
       "Sex         -2.643398\n",
       "dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta = model_parameters.params\n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've got our optimized thetas, we can update the linear part of our model to show:\n",
    "\n",
    "$$\\theta^Tx = 3.294642 * x_0 + -0.960553 * Pclass + -2.643398 * Sex$$\n",
    "\n",
    "Given the above, we can update our hypothesis and say the probability of someone surviving is:\n",
    "$$h_\\theta(x) = \\frac{1}{1 + e^{-3.294642 * x_0 + -0.960553 * Pclass + -2.643398 * Sex}}$$\n",
    "\n",
    "Let's write that in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def hypothesis(pclass, sex, theta):\n",
    "    z = theta[0] + pclass * theta[1] + sex * theta[2]\n",
    "    return 1 / (1 + exp(-1*z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've got everything set up, let's make a prediction. First we need to see how scikit-learn encoded our categorical variable Sex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['female' 'male']\n"
     ]
    }
   ],
   "source": [
    "print le_sex.inverse_transform([0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, so if we wanted to see how a woman in first class faired, we could make a prediction like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91166115474379972"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothesis(1, 0, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like she would have faired pretty well. What about a guy in third class..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.097052204431359895"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hypothesis(3, 1, theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, that's abysmal. But it makes sense, the people in the lower parts of the ship probably didn't have a very good chance of escaping when the iceberg hit and the Titanic is known for saving women and children first. Unfortunately, it looks like if you were a poor male on the Titanic, you almost certainly didn't survive - at least according to our logistic regression model :).\n",
    "\n",
    "Now there are other things we could have done like more pre-processing of the data such as engineering new features. We could have also used other variables in our logistic regression function that would have helped our model be more accurate. But now we know the basics of logistic regression - for a single class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the good news is that since we know how to pull off logistic regression for one class (binary classification), we can easily do it for multiple classes. The strategy is to just do binary classification for each class while setting the other classes as negative classes. So for each class:\n",
    "\n",
    "$$h_\\theta^{(i)}(x) = P(y = 1\\ |\\ x;\\ \\theta)\\ (i = 1, 2, 3)$$\n",
    "\n",
    "We want to train a logistic regression classifier $h_\\theta^{(i)}(x)$ for each class $i$ to predict the probability that $y = 1$.\n",
    "\n",
    "On a new input $x$, to make a prediction, pick the class $i$ that gives the highest probability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
